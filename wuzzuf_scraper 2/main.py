# main.py
import logging
from config import BASE_URL, START_PAGE, OUTPUT_FILE
from scraper import WuzzufAdvancedScraper
from storage import save_advanced_data

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù€ Logging Ù„ÙŠØ¸Ù‡Ø± Ù„Ùƒ Ù…Ø§ ÙŠØ­Ø¯Ø« ÙÙŠ Ø§Ù„Ù€ Terminal
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


def run_scraper():
    scraper = WuzzufAdvancedScraper()

    # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø§Ø¨Ø· Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ (start=0 Ù„Ù„ØµÙØ­Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ØŒ start=10 Ù„Ù„Ø«Ø§Ù†ÙŠØ©...)
    # ÙŠÙ…ÙƒÙ†Ùƒ ØªØºÙŠÙŠØ± START_PAGE Ù…Ù† Ù…Ù„Ù config
    url = f"{BASE_URL}?q=python&start={START_PAGE * 10}"

    logging.info(f"ğŸŒ Processing Page: {START_PAGE + 1}...")

    # ØªÙ… ØªØºÙŠÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ Ù‡Ù†Ø§ Ù„Ù…Ù†Ø§Ø¯Ø§Ø© Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ø´Ø§Ù…Ù„Ø©
    final_jobs_list = scraper.scrape_page_data(url)

    if final_jobs_list:
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        save_advanced_data(final_jobs_list, OUTPUT_FILE)
    else:
        logging.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§ØªØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø§ØªØµØ§Ù„ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø£Ùˆ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª.")


if __name__ == "__main__":
    run_scraper()

