# main.py
import logging
import time
from config import BASE_URL, PAGES_COUNT, OUTPUT_FILE
from scraper import WuzzufScraper
from storage import save_data_to_csv

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù„ÙˆØ¬Ù†Ø¬ Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ù‡Ù†Ø§
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)


def start_scraping():
    scraper = WuzzufScraper()
    all_jobs_data = []

    for page in range(0, PAGES_COUNT):
        # ÙÙŠ Wuzzuf ÙƒÙ„ ØµÙØ­Ø© ØªØ²ÙŠØ¯ Ø¨Ù…Ù‚Ø¯Ø§Ø± 10 (0, 10, 20...)
        url = f"{BASE_URL}?start={page * 10}"
        logging.info(f"ğŸŒ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙØ­Ø© Ø±Ù‚Ù… {page + 1}...")

        html = scraper.fetch_page(url)
        if html:
            jobs = scraper.parse_jobs(html)
            all_jobs_data.extend(jobs)
            logging.info(f"âœ… ØªÙ… Ø¬Ù…Ø¹ {len(jobs)} ÙˆØ¸ÙŠÙØ© Ù…Ù† Ø§Ù„ØµÙØ­Ø© {page + 1}")

        time.sleep(2)  # Ø§Ø­ØªØ±Ø§Ù… Ø§Ù„Ø³ÙŠØ±ÙØ±

    save_data_to_csv(all_jobs_data, OUTPUT_FILE)
    logging.info(f"ğŸ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù†ØªÙ‡Øª. Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©: {len(all_jobs_data)}")


if __name__ == "__main__":
    start_scraping()
